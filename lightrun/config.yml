# logging configures logging parameters for the collector.
logging:
  # level is the level of verbosity that the collector will emit --> Default is `info`.
  level: ${LOGGING_LEVEL:info}

# metrics configuration - configures how the collector emits metrics.
# Note: These metrics are produced by the collector itself.
metrics:
  # scope configures how metric names will be formed.
  scope:
    # prefix defines prefix for all metric names.
    prefix: "chronocollector"
    # tags define custom tags that will be attached to every metric from the collector.
    tags: {}
  # prometheus defines options for the Prometheus reporter.
  prometheus:
    # handlerPath is the route on which metrics will be served.
    handlerPath: /metrics
  # sanitization indicates that metrics will be in the Prometheus format.
  sanitization: prometheus
  # samplingRate is the rate at which metrics will be sampled. Default is 1.0.
  samplingRate: 1.0
  # extended configures extra process level metrics that will be emitted.
  # Options include: none, simple, moderate, detailed, See below link for more information on options:
  # https://bit.ly/39v2yOO
  extended: none

# wrapper configuration - configuration for the collector wrapper.
wrapper:
  # listenAddress is the address that collector wrapper will serve requests on. Currently supports /metrics if enabled.
  listenAddress: "${LISTEN_ADDRESS:0.0.0.0:3029}"
  logFilter:
    # enables the log filtering configuration.
    enabled: ${LOG_FILTERING_ENABLED:false}
  # metrics configuration - configures how the collector wrapper emits metrics.
  # Note: These metrics are produced by the collector itself.
  metrics:
    # scope configures how metric names will be formed.
    scope:
      # prefix defines prefix for all metric names.
      prefix: "chronocollector"
      # tags define custom tags that will be attached to every metric from the collector.
      tags: {}
    # prometheus defines options for the Prometheus reporter.
    prometheus:
      # handlerPath is the route on which metrics will be served.
      handlerPath: /metrics
    # sanitization indicates that metrics will be in the Prometheus format.
    sanitization: prometheus
    # samplingRate is the rate at which metrics will be sampled. Default is 1.0.
    samplingRate: 1.0
    # extended configures extra process level metrics that will be emitted.
    # Options include: none, simple, moderate, detailed, See below link for more information on options:
    # https://bit.ly/39v2yOO
    extended: none

# listenAddress is the address that collector will serve requests on. Currently supports /metrics and the remote write endpoint if enabled.
listenAddress: "${LISTEN_ADDRESS:0.0.0.0:3030}"

# labels defines key value pairs that will be appended to each metric being sent to the Chronosphere backend.
labels:
 # defaults defines the default labels.
 defaults:
   origin: "lightrun-c10e-standalone"

# backend defines what backend the collector forwards metrics to. The chronogateway is the main backend for Chronosphere. The below is needed to configure and send metrics to the `chronogateway`.
backend:
  # type controls which backend configuration to read from. It can either be `gateway` or `prometheusRemoteWrite`.
  type: ${BACKEND_TYPE:gateway}
  # annotatedMetrics controls whether the backend supports annotated metrics in which case the collector will use an annotated write request to send the metrics.
  annotatedMetrics: ${BACKEND_ANNOTATED_METRICS:false}
  gateway:
    # address is what the collector sends to `company.chronosphere.io:443`. This can also be a proxy address.
    address: ${GATEWAY_ADDRESS:""}
    # servername is the gateway server name in case address is a proxy address.
    serverName: ${GATEWAY_SERVER_NAME:""}
    # insecure determines whether or not metrics are allowed via an insecure connection.
    # Note: Setting to `false` ensures that forwarding metrics are not allowed via an insecure connection. This should only be set to `true` for development purposes.
    insecure: ${GATEWAY_INSECURE:false}
    # cert and certSkipVerify are both related to the TLS. cert allows specifying a certificate chain for the CA.
    # certSkipVerify allows skipping the TLS verification.
    # Note: both cert and certSkipVerify are hardly ever used / used primarily for tesing.
    # The config is set to use the system’s default “certificate pool”, but fields can be altered if there are special requirements or a non-standard cert setup.
    cert: ${GATEWAY_CERT:""}
    certSkipVerify: ${GATEWAY_CERT_SKIP_VERIFY:false}
    # A file containing the API token. If present will override the env var setting.
    apiTokenFile: ${API_TOKEN_FILE:""}

# In order to send metrics to the Prometheus remote write backend (vs. through the `chronogateway`), then uncomment the below section and change type to `prometheusRemoteWrite`.
# backend:
#  type: prometheusRemoteWrite
#  prometheusRemoteWrite:
# writeURL is the Prometheus remote write endpoint that metrics will be written to. This will typically be an m3coordinator.
#    writeURL: ${PROMETHEUS_REMOTE_WRITE_ADDRESS:""}
# httpClientTimeout is the deadline for the Prometheus remote write backend to accept the remote write request.
#    httpClientTimeout: ${PROMETHEUS_REMOTE_WRITE_HTTP_CLIENT_TIMEOUT:30s}
# userAgent is the HTTP user agent that the collector will send requests with.
#    userAgent: ${PROMETHEUS_REMOTE_WRITE_USER_AGENT:chronosphere.io/chronocollector}

# discovery controls how the collector will discover targets to scrape.
discovery:
  # kubernetes defines settings for discovering targets in the same Kubernetes cluster the collector is running in.
  kubernetes:
    # enabled indicates whether Kubernetes discovery is enabled.
    enabled: false
#     # serviceMonitorsEnabled indicates whether to use service monitors for job config generation.
#     serviceMonitorsEnabled: true
#     # endpointsDiscoveryEnabled discovery of endpoints. Requires service monitors to be enabled.
#     endpointsDiscoveryEnabled: true
#     # kubeSystemEndpointsDiscoveryEnabled allows additional discovery of kube system endpoints. This can get expensive.
#     kubeSystemEndpointsDiscoveryEnabled: true
#     # kubeletMonitoring defines configuration for scraping metrics from kubelet on the host that the collector is running on.
#     kubeletMonitoring:
#       # enabled indicates whether or not the kubelet will be monitored.
#       enabled: ${KUBERNETES_KUBELET_MONITORING_ENABLED:false}
#       # port indicates the port that the kubelet is running on.
#       port: 10250
#       # bearerTokenFile indicates path to file containing collector service account token.
#       bearerTokenFile: "/var/run/secrets/kubernetes.io/serviceaccount/token"
#       # labelsToAugment indicates the pod labels to augment on to the kubelet metrics
#       labelsToAugment: ["app"]
# The collector config can also be set to scrape a static Prometheus endpoint.
# Note: Kubernetes and Prometheus discovery can be enabled at the same time.
# In order to use this functionality, uncomment the below section, and make sure the target is updated to the correct endpoint.
# See the below for an example of a target that scrapes a node_exporter listening on "localhost:9100".
  prometheus:
    enabled: true
    scrape_configs:
# job_name: The job name assigned to scraped metrics by default.
    - job_name: 'lightrun_kubecon-demo'
# scrape_interval: How frequently to scrape targets from this job.
      scrape_interval: 30s
# scrape_timeout: Per-scrape timeout when scraping this job.
      scrape_timeout: 30s
      metrics_path: /actuator/prometheus
# static_configs: List of labeled statically configured targets for this job.
      static_configs:
# Note: make sure to update the target to correct endpoint.
        - targets: ['localhost:8080']

# Push configures push mechanisms to send metrics.
# prometheusRemoteWrite is configuration for prometheus remote write, that allows forwarding remote write requests to Chronosphere.
# Remote write endpoint will be available at /remote/write
# push:
# prometheusRemoteWrite is configuration for prometheus remote write, that allows forwarding remote write requests to Chronosphere.
# Remote write endpoint will be available at /remote/write
# prometheusRemoteWrite:
#   enabled: true
# statsd is configuration for sending statsd metrics to Chronosphere. It is hosted as a UDP server at the provided listen address.
# statsd:
#   enabled: true
#   listenAddress: 0.0.0.0:3031
# optional push interval controls how frequently metrics are pushed to the backend (Default 1s), if the packet size limit is not hit.
# pushInterval: 1s
# optional prefix adds a prefix to all statsd metrics pushed.
# prefix: "staging.foo"
# dogstatsd is configuration for sending dogstatsd metrics to Chronosphere. It is hosted as a UDP server at the provided listen address.
# dogstatsd:
#   enabled: true
#   listenAddress: 0.0.0.0:9125
#   labels:
#     env: "staging"
#     foo: "bar"
# optional push interval controls how frequently metrics are pushed to the backend (Default 1s), if the packet size limit is not hit.
# pushInterval: 1s
# optional labels adds a tag(s) to all dogstatsd metrics pushed.
# carbon is configuration for sending carbon metrics to Chronosphere. It is hosted as a UDP server at the provided listen address.
# carbon:
#   enabled: true
#   listenAddress: 0.0.0.0:3032
# optional push interval controls how frequently metrics are pushed to the backend (Default 1s), if the packet size limit is not hit.
# pushInterval: 1s
# optional prefix adds a prefix to all statsd metrics pushed.
# prefix: "staging.foo"

# kubernetes encapsulates configuration related to scraping metrics on Kubernetes.
# kubernetes:
#   # client configures the Kubernetes client.
#   client:
#     # outOfCluster is true if the collector is running outside of a Kubernetes client (only used for testing).
#     outOfCluster: ${KUBERNETES_CLIENT_OUT_OF_CLUSTER:false}
#   # processor defines configuration for processing pods discovered on Kubernetes.
#   processor:
#     # annotationsPrefix is the prefix for annotations that the collector will use to scrape discovered pods.
#     annotationsPrefix: ${KUBERNETES_PROCESSOR_ANNOTATIONS_PREFIX:"prometheus.io/"}

# scrape is the global scrape config. It is not tied to a single job, and can be overridden with annotations and the job config.
# Visit the Scrape Config section under Collector documentation for more information.
# scrape:
# defaults are the global scrape options when none other are configured.
#  defaults:
#    metricsPath: "/metrics"
#    scheme: "http"
#    scrapeInterval: "10s"
#    scrapeTimeout: "10s"
#    honorLabels: "false"
#    honorTimestamps: "true"
#    relabels: <relabel_config>
#    metricRelabels: <relabel_config>
#  # enableStalenessMarker enables staleness markers. Special floating point NaNs are emitted
#  # when scrapes fail or targets are removed. These events form an important signal to PromQL
#  # to stop interpolating the related series.
#  enableStalenessMarker: <true|false>
#  sanitization:
#    # dropDuplicateSeries prevents multiple series from being appended when they have identical
#    # label sets. This deduplication only applies to series for the same target.
#    dropDuplicateSeries: <true|false>

# jobs:
# job name is the name assigned to scraped metrics by default. It is used to override specific jobs if using Kubernetes annotations.
# job name must be unique across all scrape configurations.
# - name: "foo"
# Visit the Prometheus site for more information and default values for scrape config (https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config).
#   options:
#     metricsPath: <path>
#     params: <string>
#     scheme: <scheme>
#     scrapeInterval: <scrape_interval>
#     scrapeTimeout: <scrape_timeout>
#     honorLabels: <boolean>
#     honorTimestamps: <boolean>
# Visit the Prometheus site for more information and default values for relabel config (https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config).
#     relabels: <relabel_config>
#     metricRelabels: <relabel_config>
#    Optional port to override the scrape job created with.
#     port: <port>
# spans:
#   enabled: true
#   otel:
#     enabled: true
#     listenAddress: 0.0.0.0:4317
